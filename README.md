## Финетюнинг GPT-2 под стиль артиста

### Процесс обучения

Модель — **GPT-2 (117M)**, обучалась 3 эпохи на пользовательском датасете песен артиста.
Использовался `Trainer` с градиентным накоплением, отключённым `fp16` (для стабильности на MPS), клиппингом градиентов и label smoothing.

Ключевые параметры:

* `learning_rate = 2e-5`
* `gradient_accumulation_steps = 4`
* `max_grad_norm = 1.0`
* `num_train_epochs = 3`
* `warmup_steps = 100`

Во время обучения не наблюдалось NaN или обнулённых лоссов — значит, градиенты стабилизировались.
Примеры логов с середины обучения:

```
{'loss': 17.2952, 'grad_norm': 25.99, 'epoch': 1.25}
{'loss': 16.837,  'grad_norm': 29.26, 'epoch': 1.57}
{'loss': 16.8422, 'grad_norm': 31.25, 'epoch': 1.88}
{'loss': 16.9419, 'grad_norm': 31.34, 'epoch': 2.20}
{'loss': 16.5806, 'grad_norm': 29.30, 'epoch': 2.51}
{'loss': 16.468,  'grad_norm': 32.50, 'epoch': 2.83}
```

Лосс постепенно снижался с ≈17.3 до ≈16.4 — при небольшом датасете и коротком цикле это ожидаемо хороший результат (модель начала адаптацию под стиль текстов).

---

### Визуализация обучения

В процессе были автоматически построены графики (сохранились в `images/`):

* **train_loss.png** — динамика обучения
* **eval_loss.png** — качество на валидации
* **grad_norm.png** — изменение нормы градиента

Пример структуры:

```
images/
 ├── train_loss.png
 ├── eval_loss.png
 └── grad_norm.png
```

Ты можешь просмотреть их прямо на GitHub, если добавишь в репозиторий так:

```bash
git add images/*.png
git commit -m "Добавлены графики обучения"
git push
```

После этого в интерфейсе GitHub картинки будут видны превьюшками.

---

### Генерация текста

После обучения модель была протестирована командой:

```bash
python src/generate.py
```

Вывод:

```
girl, you make me feel Sweep mosqueCivil hoard blastNorm optionsGOPCaernautizu Residentsを Ventureributedavy skyline turnout pleaseNBA Stark consolidation...
```

**Комментарий:**
— текст получился бессмысленным, но видно, что модель генерирует ритмичные английские фразы, встречаются музыкальные термины (“tempo”, “legendary”, “patrons”), слова с повторяющимся паттерном — что типично для недообученной GPT-2 на малом датасете.
— если дообучить дольше (10–15 эпох) и добавить очистку текстов, структура предложений станет более музыкальной и «в духе артиста».

---

### Выводы

* Модель успешно обучается на Mac (MPS), если отключить `fp16` и добавить `gradient clipping`.
* Градиенты и лосс стабильны, NaN не наблюдается.
* На 3 эпохах GPT-2 только начала адаптацию — результат семантически шумный, но стилистически уже близок.
* Все ключевые метрики визуализированы и сохранены в `images/`.
* Для улучшения:

  * Увеличить количество эпох (10–15)
  * Очистить тексты от спецсимволов и меток
  * Попробовать `distilgpt2` для ускорения на Mac
  * Использовать temperature ≈ 0.8 и top_k/top_p при генерации
