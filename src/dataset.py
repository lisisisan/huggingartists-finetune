import torch
from datasets import load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from pathlib import Path

def train_model():
    dataset_path = "data/mj_dataset"
    model_name = "gpt2"
    output_dir = "models/mj-gpt2"
    log_dir = "runs/mj-gpt2"

    print("üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...")
    ds = load_from_disk(dataset_path)

    print("üî† –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    def tokenize(batch):
        return tokenizer(batch["train"], truncation=True, padding="max_length", max_length=128)

    print("üß© –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    tokenized_ds = ds.map(tokenize, batched=True, remove_columns=["train"])

    print("ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    model = AutoModelForCausalLM.from_pretrained(model_name)

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è TrainingArguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        save_strategy="epoch",  # –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å ‚Äî –≤—Å—ë –µ—â—ë –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        learning_rate=5e-5,
        warmup_steps=50,
        report_to="tensorboard",  # –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard
        logging_dir=log_dir,
        logging_steps=50,
        fp16=False,  # fp16 –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ MPS
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_ds["train"],
        eval_dataset=tokenized_ds["validation"],
        data_collator=data_collator,
    )

    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    trainer.train()

    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")

if __name__ == "__main__":
    train_model()
